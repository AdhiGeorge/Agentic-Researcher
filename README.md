# Agentic Researcher

An advanced AI research assistant built using OpenAI Swarm framework that can understand research queries, gather information from the web, validate answers, and store knowledge for future reference.

## Features

- ğŸ¤– **Multi-Agent Architecture**: Uses OpenAI Swarm for intelligent agent coordination
- ğŸ” **Web Research**: DuckDuckGo search with Playwright web scraping
- ğŸ§  **Knowledge Base**: Local Qdrant vector database for storing and retrieving information
- âœ… **Answer Validation**: Review/Reflection agent ensures answer quality before delivery
- ğŸ’» **Code Execution**: Terminal runner for executing generated code
- ğŸ—„ï¸ **Data Persistence**: SQLite database for project management and logs
- ğŸ”§ **Azure OpenAI**: GPT-4o integration via Azure OpenAI

## Architecture

The system consists of several specialized agents:

1. **Planner Agent**: Breaks down research queries into actionable steps
2. **Researcher Agent**: Performs web searches and information gathering
3. **Coder Agent**: Generates code when needed
4. **Review Agent**: Validates and refines answers before delivery
5. **Runner Agent**: Executes generated code in a safe environment

## Requirements

- Python 3.10+
- Azure OpenAI API key
- Qdrant (local installation)
- Playwright browsers

## Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Install Playwright browsers:
   ```bash
   playwright install --with-deps
   ```
4. Set up your configuration in `config.toml`

## Usage

```bash
python main.py
```

## Configuration

Create a `config.toml` file with your Azure OpenAI settings:

```toml
[azure_openai]
api_key = "your-azure-openai-api-key"
endpoint = "https://your-resource.openai.azure.com/"
api_version = "2024-02-15-preview"
deployment_name = "gpt-4o"

[qdrant]
host = "localhost"
port = 6333
collection_name = "research_knowledge"

[database]
path = "data/research.db"
```

## Project Structure

```
agentic-researcher/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ planner.py
â”‚   â”‚   â”œâ”€â”€ researcher.py
â”‚   â”‚   â”œâ”€â”€ coder.py
â”‚   â”‚   â”œâ”€â”€ reviewer.py
â”‚   â”‚   â””â”€â”€ runner.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â””â”€â”€ knowledge_base.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ web_search.py
â”‚   â”‚   â”œâ”€â”€ code_executor.py
â”‚   â”‚   â””â”€â”€ azure_client.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ data/
â”œâ”€â”€ logs/
â”œâ”€â”€ temp/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ config.toml
```

## Web Search Pipeline

Agentres uses a robust, multi-fallback web search pipeline:

1. **DuckDuckGo Search** (primary)
2. **Tavily API** (secondary, requires `TAVILY_API_KEY` in `.env`)
3. **Google Custom Search API** (tertiary, requires `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` in `.env`)

For each research sub-query:
- The query is sanitized to remove code block markers, brackets, and extraneous punctuation, and to ensure only meaningful queries are sent to search.
- Up to 5 ranked URLs are retrieved (no snippets).
- The system attempts to scrape the first URL; if it fails, it tries the next, and so on.
- Only if all scraping fails for all URLs does it fall back to using search snippets.

### Configuration
- All search settings (max_results, timeout, API keys) are managed in `config.yaml` and `.env`. 